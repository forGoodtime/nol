#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ —Å Ground Truth –¥–∞–Ω–Ω—ã–º–∏
"""

import argparse
import json
import os
from pathlib import Path
from typing import Dict, List, Tuple

import numpy as np
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns


class GTComparator:
    """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å Ground Truth –¥–∞–Ω–Ω—ã–º–∏"""
    
    def __init__(self, results_dir: str, gt_dir: str):
        self.results_dir = Path(results_dir)
        self.gt_dir = Path(gt_dir)
        
        self.predictions = {}
        self.ground_truth = {}
        
        self.damage_labels = {
            0: "no_damage",
            1: "light_damage",
            2: "moderate_damage", 
            3: "severe_damage"
        }
    
    def load_predictions(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –º–æ–¥–µ–ª–∏"""
        print("üìä –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π...")
        
        for result_file in self.results_dir.rglob("*_result.json"):
            try:
                with open(result_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    image_name = data['image_name']
                    self.predictions[image_name] = {
                        'damage_level': data['predictions']['damage_level'],
                        'confidence': data['predictions']['confidence'],
                        'damage_types': data['predictions'].get('damage_types', [])
                    }
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {result_file}: {e}")
        
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.predictions)} –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π")
    
    def load_ground_truth(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ Ground Truth –¥–∞–Ω–Ω—ã—Ö"""
        print("üéØ –ó–∞–≥—Ä—É–∑–∫–∞ Ground Truth...")
        
        # –ò—â–µ–º —Ñ–∞–π–ª—ã —Å –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏
        gt_files = list(self.gt_dir.rglob("*.json"))
        
        if not gt_files:
            print(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ GT —Ñ–∞–π–ª–æ–≤ –≤ {self.gt_dir}")
            return False
        
        for gt_file in gt_files:
            try:
                with open(gt_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                    # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ GT
                    if 'images' in data and 'annotations' in data:
                        # COCO —Ñ–æ—Ä–º–∞—Ç
                        self._load_coco_gt(data)
                    elif 'ground_truth' in data:
                        # –ö–∞—Å—Ç–æ–º–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
                        self._load_custom_gt(data)
                    else:
                        # –ü—Ä–æ—Å—Ç–æ–π —Ñ–æ—Ä–º–∞—Ç: {image_name: {damage_level: X, ...}}
                        for image_name, gt_data in data.items():
                            if isinstance(gt_data, dict) and 'damage_level' in gt_data:
                                self.ground_truth[image_name] = gt_data
                                
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ GT {gt_file}: {e}")
        
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.ground_truth)} GT –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π")
        return len(self.ground_truth) > 0
    
    def _load_coco_gt(self, coco_data):
        """–ó–∞–≥—Ä—É–∑–∫–∞ COCO —Ñ–æ—Ä–º–∞—Ç–∞"""
        # –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ image_id -> filename
        image_map = {img['id']: img['file_name'] for img in coco_data['images']}
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º
        image_annotations = {}
        for ann in coco_data['annotations']:
            image_id = ann['image_id']
            if image_id not in image_annotations:
                image_annotations[image_id] = []
            image_annotations[image_id].append(ann)
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –Ω–∞—à —Ñ–æ—Ä–º–∞—Ç
        for image_id, annotations in image_annotations.items():
            image_name = image_map.get(image_id, f"image_{image_id}")
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏—è
            max_damage_level = 0
            damage_types = set()
            
            for ann in annotations:
                category_id = ann.get('category_id', 0)
                # –ú–∞–ø–ø–∏–Ω–≥ category_id –≤ damage_level (–Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è)
                damage_level = min(category_id, 3)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 3
                max_damage_level = max(max_damage_level, damage_level)
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–∏–ø –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏—è –∏–∑ attributes
                if 'attributes' in ann:
                    damage_types.update(ann['attributes'].get('damage_types', []))
            
            self.ground_truth[image_name] = {
                'damage_level': max_damage_level,
                'damage_types': list(damage_types)
            }
    
    def _load_custom_gt(self, data):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞"""
        for gt_item in data['ground_truth']:
            image_name = gt_item['image_name']
            self.ground_truth[image_name] = {
                'damage_level': gt_item['damage_level'],
                'damage_types': gt_item.get('damage_types', [])
            }
    
    def compare_predictions(self) -> Dict:
        """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π —Å GT"""
        if not self.predictions or not self.ground_truth:
            print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")
            return {}
        
        # –ù–∞—Ö–æ–¥–∏–º –æ–±—â–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        common_images = set(self.predictions.keys()) & set(self.ground_truth.keys())
        
        if not common_images:
            print("‚ùå –ù–µ—Ç –æ–±—â–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –º–µ–∂–¥—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏ –∏ GT")
            print(f"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {list(self.predictions.keys())[:5]}...")
            print(f"GT: {list(self.ground_truth.keys())[:5]}...")
            return {}
        
        print(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(common_images)} –æ–±—â–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
        
        # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        y_true = []
        y_pred = []
        confidences = []
        
        detailed_results = []
        
        for image_name in common_images:
            gt_level = self.ground_truth[image_name]['damage_level']
            pred_level = self.predictions[image_name]['damage_level']
            confidence = self.predictions[image_name]['confidence']
            
            y_true.append(gt_level)
            y_pred.append(pred_level)
            confidences.append(confidence)
            
            detailed_results.append({
                'image_name': image_name,
                'gt_level': gt_level,
                'pred_level': pred_level,
                'confidence': confidence,
                'correct': gt_level == pred_level
            })
        
        # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        accuracy = accuracy_score(y_true, y_pred)
        
        # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
        cm = confusion_matrix(y_true, y_pred, labels=[0, 1, 2, 3])
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–π –æ—Ç—á–µ—Ç
        class_report = classification_report(
            y_true, y_pred, 
            target_names=[self.damage_labels[i] for i in range(4)],
            output_dict=True
        )
        
        # –ê–Ω–∞–ª–∏–∑ –ø–æ —É—Ä–æ–≤–Ω—è–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        confidence_analysis = self._analyze_by_confidence(
            detailed_results, confidences
        )
        
        results = {
            'total_images': len(common_images),
            'accuracy': accuracy,
            'confusion_matrix': cm.tolist(),
            'classification_report': class_report,
            'confidence_analysis': confidence_analysis,
            'detailed_results': detailed_results
        }
        
        return results
    
    def _analyze_by_confidence(self, detailed_results: List[Dict], 
                             confidences: List[float]) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø–æ —É—Ä–æ–≤–Ω—è–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏"""
        confidence_thresholds = [0.5, 0.7, 0.8, 0.9]
        analysis = {}
        
        for threshold in confidence_thresholds:
            high_conf_results = [r for r, c in zip(detailed_results, confidences) 
                               if c >= threshold]
            
            if high_conf_results:
                correct = sum(1 for r in high_conf_results if r['correct'])
                accuracy = correct / len(high_conf_results)
                
                analysis[f'confidence_{threshold}'] = {
                    'count': len(high_conf_results),
                    'accuracy': accuracy,
                    'percentage_of_total': len(high_conf_results) / len(detailed_results) * 100
                }
        
        return analysis
    
    def create_visualizations(self, results: Dict, output_dir: str):
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # 1. –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
        plt.figure(figsize=(10, 8))
        
        cm = np.array(results['confusion_matrix'])
        
        plt.subplot(2, 2, 1)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=[self.damage_labels[i] for i in range(4)],
                    yticklabels=[self.damage_labels[i] for i in range(4)])
        plt.title(f'–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫\n–¢–æ—á–Ω–æ—Å—Ç—å: {results["accuracy"]:.3f}')
        plt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ')
        plt.ylabel('Ground Truth')
        
        # 2. –¢–æ—á–Ω–æ—Å—Ç—å –ø–æ —É—Ä–æ–≤–Ω—è–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        plt.subplot(2, 2, 2)
        conf_analysis = results['confidence_analysis']
        thresholds = []
        accuracies = []
        counts = []
        
        for key, data in conf_analysis.items():
            threshold = float(key.split('_')[1])
            thresholds.append(threshold)
            accuracies.append(data['accuracy'])
            counts.append(data['count'])
        
        plt.plot(thresholds, accuracies, 'bo-', label='–¢–æ—á–Ω–æ—Å—Ç—å')
        plt.xlabel('–ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏')
        plt.ylabel('–¢–æ—á–Ω–æ—Å—Ç—å')
        plt.title('–¢–æ—á–Ω–æ—Å—Ç—å vs –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å')
        plt.grid(True, alpha=0.3)
        plt.legend()
        
        # 3. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫
        plt.subplot(2, 2, 3)
        detailed = results['detailed_results']
        correct_conf = [r['confidence'] for r in detailed if r['correct']]
        incorrect_conf = [r['confidence'] for r in detailed if not r['correct']]
        
        plt.hist(incorrect_conf, bins=20, alpha=0.7, label='–û—à–∏–±–∫–∏', color='red')
        plt.hist(correct_conf, bins=20, alpha=0.7, label='–ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ', color='green')
        plt.xlabel('–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å')
        plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ')
        plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏')
        plt.legend()
        
        # 4. –î–∏–∞–≥—Ä–∞–º–º–∞ –ø–æ –∫–ª–∞—Å—Å–∞–º
        plt.subplot(2, 2, 4)
        class_report = results['classification_report']
        classes = [self.damage_labels[i] for i in range(4)]
        f1_scores = [class_report[cls]['f1-score'] for cls in classes 
                    if cls in class_report]
        
        plt.bar(range(len(f1_scores)), f1_scores, color='skyblue')
        plt.xlabel('–ö–ª–∞—Å—Å –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏—è')
        plt.ylabel('F1-Score')
        plt.title('F1-Score –ø–æ –∫–ª–∞—Å—Å–∞–º')
        plt.xticks(range(len(f1_scores)), classes, rotation=45)
        
        plt.tight_layout()
        plt.savefig(output_path / 'gt_comparison.png', dpi=300, bbox_inches='tight')
        print(f"üìä –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {output_path / 'gt_comparison.png'}")
        plt.close()
    
    def generate_report(self, results: Dict, output_file: str):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
        if not results:
            return
        
        report = []
        report.append("=" * 80)
        report.append("–û–¢–ß–ï–¢ –°–†–ê–í–ù–ï–ù–ò–Ø –° GROUND TRUTH")
        report.append("=" * 80)
        report.append(f"–í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è: {results['total_images']}")
        report.append(f"–û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {results['accuracy']:.3f} ({results['accuracy']*100:.1f}%)")
        report.append("")
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–π –æ—Ç—á–µ—Ç
        report.append("–ú–ï–¢–†–ò–ö–ò –ü–û –ö–õ–ê–°–°–ê–ú:")
        class_report = results['classification_report']
        for class_name in [self.damage_labels[i] for i in range(4)]:
            if class_name in class_report:
                metrics = class_report[class_name]
                report.append(f"  {class_name}:")
                report.append(f"    Precision: {metrics['precision']:.3f}")
                report.append(f"    Recall: {metrics['recall']:.3f}")
                report.append(f"    F1-Score: {metrics['f1-score']:.3f}")
                report.append(f"    Support: {int(metrics['support'])}")
        report.append("")
        
        # –ê–Ω–∞–ª–∏–∑ –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        report.append("–ê–ù–ê–õ–ò–ó –ü–û –£–†–û–í–ù–Ø–ú –£–í–ï–†–ï–ù–ù–û–°–¢–ò:")
        conf_analysis = results['confidence_analysis']
        for key in sorted(conf_analysis.keys()):
            threshold = key.split('_')[1]
            data = conf_analysis[key]
            report.append(f"  –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å >= {threshold}:")
            report.append(f"    –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ: {data['count']} ({data['percentage_of_total']:.1f}%)")
            report.append(f"    –¢–æ—á–Ω–æ—Å—Ç—å: {data['accuracy']:.3f}")
        report.append("")
        
        # –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫
        detailed = results['detailed_results']
        errors = [r for r in detailed if not r['correct']]
        
        if errors:
            report.append("–ê–ù–ê–õ–ò–ó –û–®–ò–ë–û–ö:")
            report.append(f"  –í—Å–µ–≥–æ –æ—à–∏–±–æ–∫: {len(errors)}")
            
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ –ø–æ —Ç–∏–ø–∞–º
            error_patterns = {}
            for error in errors:
                pattern = f"GT:{error['gt_level']} ‚Üí Pred:{error['pred_level']}"
                if pattern not in error_patterns:
                    error_patterns[pattern] = []
                error_patterns[pattern].append(error)
            
            report.append("  –ß–∞—Å—Ç—ã–µ –æ—à–∏–±–∫–∏:")
            for pattern, pattern_errors in sorted(error_patterns.items(), 
                                                key=lambda x: len(x[1]), reverse=True):
                avg_conf = sum(e['confidence'] for e in pattern_errors) / len(pattern_errors)
                report.append(f"    {pattern}: {len(pattern_errors)} —Å–ª—É—á–∞–µ–≤ "
                             f"(—Å—Ä–µ–¥. —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {avg_conf:.3f})")
        
        report.append("")
        report.append("=" * 80)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report))
        
        print(f"üìù –û—Ç—á–µ—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_file}")
        
        # –í—ã–≤–æ–¥–∏–º –≤ –∫–æ–Ω—Å–æ–ª—å
        print("\n" + "\n".join(report))


def main():
    parser = argparse.ArgumentParser(description="–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å Ground Truth")
    parser.add_argument("--results-dir", type=str,
                       default="data/real_test_cases/results",
                       help="–ü–∞–ø–∫–∞ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞")
    parser.add_argument("--gt-dir", type=str,
                       default="data/annotations",
                       help="–ü–∞–ø–∫–∞ —Å Ground Truth –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏")
    parser.add_argument("--output-dir", type=str,
                       default="data/real_test_cases/gt_comparison",
                       help="–ü–∞–ø–∫–∞ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")
    
    args = parser.parse_args()
    
    project_root = Path(__file__).parent.parent
    results_dir = project_root / args.results_dir
    gt_dir = project_root / args.gt_dir
    output_dir = project_root / args.output_dir
    
    if not results_dir.exists():
        print(f"‚ùå –ü–∞–ø–∫–∞ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {results_dir}")
        return
    
    if not gt_dir.exists():
        print(f"‚ùå –ü–∞–ø–∫–∞ —Å GT –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {gt_dir}")
        return
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ–º–ø–∞—Ä–∞—Ç–æ—Ä
    comparator = GTComparator(str(results_dir), str(gt_dir))
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    comparator.load_predictions()
    if not comparator.load_ground_truth():
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å Ground Truth –¥–∞–Ω–Ω—ã–µ")
        return
    
    # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º
    results = comparator.compare_predictions()
    
    if not results:
        print("‚ùå –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å")
        return
    
    # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
    comparator.create_visualizations(results, str(output_dir))
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
    report_file = output_dir / "gt_comparison_report.txt"
    comparator.generate_report(results, str(report_file))
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    results_file = output_dir / "detailed_comparison.json"
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"‚úÖ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤: {output_dir}")


if __name__ == "__main__":
    main()
