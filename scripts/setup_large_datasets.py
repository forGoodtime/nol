#!/usr/bin/env python3
"""–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π"""

import os
import requests
import zipfile
import tarfile
import shutil
import json
import yaml
from pathlib import Path
import cv2
import numpy as np
from PIL import Image
import glob

def download_file(url: str, filename: str, chunk_size: int = 8192) -> bool:
    """–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º"""
    try:
        print(f"üîÑ –°–∫–∞—á–∏–≤–∞–µ–º: {url}")
        response = requests.get(url, stream=True)
        response.raise_for_status()
        
        total_size = int(response.headers.get('content-length', 0))
        downloaded = 0
        
        with open(filename, 'wb') as f:
            for chunk in response.iter_content(chunk_size=chunk_size):
                if chunk:
                    f.write(chunk)
                    downloaded += len(chunk)
                    if total_size > 0:
                        percent = (downloaded / total_size) * 100
                        print(f"\rüì¶ –ü—Ä–æ–≥—Ä–µ—Å—Å: {percent:.1f}% ({downloaded}/{total_size} bytes)", end='')
        
        print(f"\n‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {filename}")
        return True
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {url}: {str(e)}")
        return False

def extract_archive(archive_path: str, extract_to: str) -> bool:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞—Ä—Ö–∏–≤–∞"""
    try:
        print(f"üìÇ –ò–∑–≤–ª–µ–∫–∞–µ–º –∞—Ä—Ö–∏–≤: {archive_path}")
        
        if archive_path.endswith('.zip'):
            with zipfile.ZipFile(archive_path, 'r') as zip_ref:
                zip_ref.extractall(extract_to)
        elif archive_path.endswith(('.tar.gz', '.tgz')):
            with tarfile.open(archive_path, 'r:gz') as tar_ref:
                tar_ref.extractall(extract_to)
        elif archive_path.endswith('.tar'):
            with tarfile.open(archive_path, 'r') as tar_ref:
                tar_ref.extractall(extract_to)
        else:
            print(f"‚ùå –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç –∞—Ä—Ö–∏–≤–∞: {archive_path}")
            return False
        
        print(f"‚úÖ –ê—Ä—Ö–∏–≤ –∏–∑–≤–ª–µ—á—ë–Ω –≤: {extract_to}")
        return True
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ {archive_path}: {str(e)}")
        return False

def setup_car_damage_datasets(project_dir: str):
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π"""
    
    data_dir = os.path.join(project_dir, "data", "large_datasets")
    os.makedirs(data_dir, exist_ok=True)
    
    print("üöó –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π")
    print("=" * 60)
    
    # –°–ø–∏—Å–æ–∫ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
    datasets = [
        {
            'name': 'Car Damage Dataset (Kaggle)',
            'description': '–ë–æ–ª—å—à–æ–π –¥–∞—Ç–∞—Å–µ—Ç —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Ç–∏–ø–∞–º–∏ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π',
            'size': '~2GB',
            'images': '~9000',
            'categories': ['scratch', 'dent', 'glass_shatter', 'lamp_broken', 'tire_flat']
        },
        {
            'name': 'Vehicle Damage Assessment',
            'description': '–î–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π',
            'size': '~1.5GB', 
            'images': '~6000',
            'categories': ['minor', 'moderate', 'severe']
        },
        {
            'name': 'CarDD (Car Damage Detection)',
            'description': '–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏',
            'size': '~3GB',
            'images': '~12000', 
            'categories': ['bumper_dent', 'door_dent', 'scratch', 'glass_crack', 'headlight']
        }
    ]
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã
    print("üìä –î–æ—Å—Ç—É–ø–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã:")
    for i, dataset in enumerate(datasets, 1):
        print(f"\n{i}. {dataset['name']}")
        print(f"   üìù {dataset['description']}")
        print(f"   üì¶ –†–∞–∑–º–µ—Ä: {dataset['size']}")
        print(f"   üñºÔ∏è  –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {dataset['images']}")
        print(f"   üè∑Ô∏è  –ö–∞—Ç–µ–≥–æ—Ä–∏–∏: {', '.join(dataset['categories'])}")
    
    return data_dir

def create_synthetic_car_damage_dataset(project_dir: str, num_images: int = 1000):
    """–°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏"""
    
    print(f"\nüé® –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ ({num_images} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π)")
    
    dataset_dir = os.path.join(project_dir, "data", "synthetic_car_damage")
    images_dir = os.path.join(dataset_dir, "images")
    labels_dir = os.path.join(dataset_dir, "labels")
    
    os.makedirs(images_dir, exist_ok=True)
    os.makedirs(labels_dir, exist_ok=True)
    
    # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π
    damage_categories = {
        0: 'no_damage',
        1: 'rust',
        2: 'dent', 
        3: 'scratch',
        4: 'severe_damage',
        5: 'missing_part'
    }
    
    # –°–æ–∑–¥–∞—ë–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
    print("üîÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö...")
    
    created_images = 0
    for i in range(num_images):
        # –°–æ–∑–¥–∞—ë–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∞–≤—Ç–æ–º–æ–±–∏–ª—è
        img_width, img_height = 640, 480
        image = np.zeros((img_height, img_width, 3), dtype=np.uint8)
        
        # –†–∏—Å—É–µ–º "–∞–≤—Ç–æ–º–æ–±–∏–ª—å" (–ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω–∏–∫)
        car_color = np.random.randint(50, 200, 3)
        cv2.rectangle(image, (100, 150), (540, 350), car_color.tolist(), -1)
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏—è
        annotations = []
        num_damages = np.random.randint(0, 4)  # 0-3 –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏—è –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        
        for _ in range(num_damages):
            # –°–ª—É—á–∞–π–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏—è (–∏—Å–∫–ª—é—á–∞–µ–º no_damage)
            damage_type = np.random.randint(1, 6)
            
            # –°–ª—É—á–∞–π–Ω–æ–µ —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏—è –Ω–∞ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ
            x = np.random.randint(120, 500)
            y = np.random.randint(170, 320)
            w = np.random.randint(20, 80)
            h = np.random.randint(20, 60)
            
            # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ bbox –Ω–µ –≤—ã—Ö–æ–¥–∏—Ç –∑–∞ –≥—Ä–∞–Ω–∏—Ü—ã
            x = max(0, min(x, img_width - w))
            y = max(0, min(y, img_height - h))
            
            # –†–∏—Å—É–µ–º –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–µ
            if damage_type == 1:  # rust
                cv2.rectangle(image, (x, y), (x+w, y+h), (0, 100, 200), -1)
            elif damage_type == 2:  # dent
                cv2.ellipse(image, (x+w//2, y+h//2), (w//2, h//2), 0, 0, 360, (50, 50, 50), -1)
            elif damage_type == 3:  # scratch
                cv2.line(image, (x, y+h//2), (x+w, y+h//2), (200, 200, 200), 3)
            elif damage_type == 4:  # severe_damage
                cv2.rectangle(image, (x, y), (x+w, y+h), (0, 0, 255), -1)
            elif damage_type == 5:  # missing_part
                cv2.rectangle(image, (x, y), (x+w, y+h), (0, 0, 0), -1)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é –≤ —Ñ–æ—Ä–º–∞—Ç–µ YOLO
            x_center = (x + w/2) / img_width
            y_center = (y + h/2) / img_height
            width = w / img_width
            height = h / img_height
            
            annotations.append(f"{damage_type} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        img_filename = f"synthetic_{i:06d}.jpg"
        img_path = os.path.join(images_dir, img_filename)
        cv2.imwrite(img_path, image)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
        txt_filename = f"synthetic_{i:06d}.txt"
        txt_path = os.path.join(labels_dir, txt_filename)
        with open(txt_path, 'w') as f:
            f.write('\n'.join(annotations))
        
        created_images += 1
        if (i + 1) % 100 == 0:
            print(f"üìä –°–æ–∑–¥–∞–Ω–æ: {i + 1}/{num_images}")
    
    # –°–æ–∑–¥–∞—ë–º dataset.yaml
    dataset_config = {
        'path': dataset_dir,
        'train': 'images',
        'val': 'images',  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ –∂–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        'nc': 6,
        'names': damage_categories
    }
    
    dataset_yaml_path = os.path.join(dataset_dir, "dataset.yaml")
    with open(dataset_yaml_path, 'w') as f:
        yaml.dump(dataset_config, f, default_flow_style=False)
    
    print(f"‚úÖ –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç —Å–æ–∑–¥–∞–Ω: {dataset_dir}")
    print(f"üìä –í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {created_images}")
    print(f"‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {dataset_yaml_path}")
    
    return dataset_yaml_path

def download_real_car_damage_dataset(project_dir: str):
    """–ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ä–µ–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç (GitHub/Kaggle)"""
    
    print("\nüåê –ü–æ–∏—Å–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –≤ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö...")
    
    # –°–ø–∏—Å–æ–∫ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤ —Å –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π
    github_datasets = [
        {
            'name': 'CarDD-Detection-Dataset',
            'url': 'https://github.com/Jia-Research-Lab/CarDD-Detection-Dataset',
            'description': 'Car Damage Detection Dataset'
        },
        {
            'name': 'vehicle-damage-assessment',
            'url': 'https://github.com/neokt/vehicle-damage-assessment',
            'description': 'Vehicle Damage Assessment using Deep Learning'
        },
        {
            'name': 'car-damage-detection',
            'url': 'https://github.com/vijayabhaskar96/Car-damage-detection',
            'description': 'Car Damage Detection using ML/DL'
        }
    ]
    
    print("üìã –ù–∞–π–¥–µ–Ω–Ω—ã–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ —Å –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏:")
    for i, repo in enumerate(github_datasets, 1):
        print(f"{i}. {repo['name']}")
        print(f"   üîó {repo['url']}")
        print(f"   üìù {repo['description']}\n")
    
    # –ü—Ä–æ–±—É–µ–º —Å–æ–∑–¥–∞—Ç—å —Å–∫—Ä–∏–ø—Ç—ã –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
    download_dir = os.path.join(project_dir, "data", "downloads")
    os.makedirs(download_dir, exist_ok=True)
    
    # –°–æ–∑–¥–∞—ë–º —Å–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
    download_script = os.path.join(download_dir, "download_datasets.sh")
    
    script_content = """#!/bin/bash
# –°–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π

echo "üöó –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π"

# –°–æ–∑–¥–∞—ë–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
mkdir -p real_datasets
cd real_datasets

# –ö–ª–æ–Ω–∏—Ä—É–µ–º —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ —Å –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏
echo "üì¶ –ö–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤..."

# CarDD Dataset
if [ ! -d "CarDD-Detection-Dataset" ]; then
    echo "–ó–∞–≥—Ä—É–∂–∞–µ–º CarDD Dataset..."
    git clone https://github.com/Jia-Research-Lab/CarDD-Detection-Dataset.git
else
    echo "CarDD Dataset —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç"
fi

# Vehicle Damage Assessment
if [ ! -d "vehicle-damage-assessment" ]; then
    echo "–ó–∞–≥—Ä—É–∂–∞–µ–º Vehicle Damage Assessment..."
    git clone https://github.com/neokt/vehicle-damage-assessment.git
else
    echo "Vehicle Damage Assessment —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç"
fi

# Car Damage Detection
if [ ! -d "car-damage-detection" ]; then
    echo "–ó–∞–≥—Ä—É–∂–∞–µ–º Car Damage Detection..."
    git clone https://github.com/vijayabhaskar96/Car-damage-detection.git
else
    echo "Car Damage Detection —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç"  
fi

echo "‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!"
echo "üìÅ –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–∞–ø–∫—É real_datasets/"
"""
    
    with open(download_script, 'w') as f:
        f.write(script_content)
    
    # –î–µ–ª–∞–µ–º —Å–∫—Ä–∏–ø—Ç –∏—Å–ø–æ–ª–Ω—è–µ–º—ã–º
    os.chmod(download_script, 0o755)
    
    print(f"üìù –°–æ–∑–¥–∞–Ω —Å–∫—Ä–∏–ø—Ç –∑–∞–≥—Ä—É–∑–∫–∏: {download_script}")
    print("üí° –ó–∞–ø—É—Å—Ç–∏—Ç–µ: bash data/downloads/download_datasets.sh")
    
    return download_script

def prepare_large_training_config(project_dir: str, dataset_yaml_path: str):
    """–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –±–æ–ª—å—à–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ"""
    
    training_config = {
        'model_config': {
            'architecture': 'yolov8s',  # –°—Ä–µ–¥–Ω—è—è –º–æ–¥–µ–ª—å –¥–ª—è –±–æ–ª—å—à–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
            'num_classes': 6,
            'input_size': 640,
            'pretrained': True,
            'freeze_backbone': False,  # –ù–µ –∑–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º –¥–ª—è –±–æ–ª—å—à–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
            'freeze_epochs': 0
        },
        'training_config': {
            'batch_size': 16,  # –ë–æ–ª—å—à–∏–π batch –¥–ª—è –±–æ–ª—å—à–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
            'epochs': 200,
            'learning_rate': 0.01,
            'weight_decay': 0.0005,
            'patience': 30,
            'save_best_only': True,
            'early_stopping': True,
            'warmup_epochs': 5,
            'cos_lr': True  # Cosine learning rate scheduling
        },
        'data_config': {
            'dataset_yaml': dataset_yaml_path,
            'num_workers': 8,  # –ë–æ–ª—å—à–µ workers –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
            'pin_memory': True,
            'cache_images': False  # –ù–µ –∫–µ—à–∏—Ä—É–µ–º –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
        },
        'augmentation_config': {
            # –°–∏–ª—å–Ω—ã–µ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–ª—è –±–æ–ª—å—à–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
            'horizontal_flip': 0.5,
            'vertical_flip': 0.1,
            'rotation': 15,
            'brightness': 0.4,
            'contrast': 0.4,
            'saturation': 0.7,
            'hue': 0.015,
            'mosaic': 1.0,  # –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º mosaic
            'mixup': 0.15,
            'copy_paste': 0.3,
            'degrees': 10.0,
            'translate': 0.1,
            'scale': 0.5,
            'shear': 2.0,
            'perspective': 0.0003,
            'flipud': 0.0,
            'fliplr': 0.5
        },
        'loss_config': {
            'use_knout_pryanik': True,
            'focal_loss_alpha': 0.25,
            'focal_loss_gamma': 2.0,
            'box_loss_gain': 7.5,
            'cls_loss_gain': 0.5,
            'obj_loss_gain': 1.0,
            'label_smoothing': 0.1  # –î–æ–±–∞–≤–ª—è–µ–º label smoothing
        },
        'validation_config': {
            'conf_threshold': 0.001,  # –ù–∏–∑–∫–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏
            'iou_threshold': 0.6,
            'save_predictions': True,
            'visualize_results': True,
            'max_det': 300
        },
        'logging_config': {
            'log_level': 'INFO',
            'log_interval': 50,
            'save_checkpoints': True,
            'tensorboard_logging': True,
            'wandb_logging': True,  # –í–∫–ª—é—á–∞–µ–º W&B –¥–ª—è –±–æ–ª—å—à–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
            'save_period': 10  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∂–¥—ã–µ 10 —ç–ø–æ—Ö
        }
    }
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    config_path = os.path.join(project_dir, 'configs', 'large_dataset_training_config.yaml')
    
    with open(config_path, 'w', encoding='utf-8') as f:
        yaml.dump(training_config, f, default_flow_style=False, allow_unicode=True, indent=2)
    
    print(f"‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –±–æ–ª—å—à–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞: {config_path}")
    
    return config_path

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    
    project_dir = "/Users/bekzat/projects/AIinDrive"
    
    print("üöó –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π")
    print("=" * 70)
    
    # 1. –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã
    data_dir = setup_car_damage_datasets(project_dir)
    
    # 2. –°–æ–∑–¥–∞—ë–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    synthetic_dataset_yaml = create_synthetic_car_damage_dataset(project_dir, num_images=1000)
    
    # 3. –ì–æ—Ç–æ–≤–∏–º —Å–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
    download_script = download_real_car_damage_dataset(project_dir)
    
    # 4. –°–æ–∑–¥–∞—ë–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –±–æ–ª—å—à–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ
    large_config_path = prepare_large_training_config(project_dir, synthetic_dataset_yaml)
    
    print("\n" + "=" * 70)
    print("‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    print("\nüìã –î–æ—Å—Ç—É–ø–Ω—ã–µ –æ–ø—Ü–∏–∏:")
    print("1. üé® –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç –≥–æ—Ç–æ–≤ (1000 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π)")
    print(f"   –ü—É—Ç—å: {synthetic_dataset_yaml}")
    print("\n2. üåê –î–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤:")
    print(f"   bash {download_script}")
    print("\n3. üöÄ –î–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö:")
    print("   python scripts/train_on_large_dataset.py")
    print("\n4. ‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –±–æ–ª—å—à–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –≥–æ—Ç–æ–≤–∞:")
    print(f"   {large_config_path}")
    
    return {
        'synthetic_dataset': synthetic_dataset_yaml,
        'download_script': download_script,
        'config_path': large_config_path
    }

if __name__ == "__main__":
    results = main()
