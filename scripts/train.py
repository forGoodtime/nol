#!/usr/bin/env python3
"""–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–µ—Ç–µ–∫—Ü–∏–∏ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π"""

import os
import sys
import yaml
import torch
import logging
from pathlib import Path
from datetime import datetime
import json

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ Python path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.trainers.damage_trainer import DamageTrainer
from src.utils.config_loader import load_config
from src.utils.logger import setup_logger

def check_small_dataset_setup(config):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–ª—è –º–∞–ª–µ–Ω—å–∫–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    
    # –°—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    data_dir = config.get('data_config', {}).get('dataset_path', 'data/curated')
    if os.path.exists(data_dir):
        image_files = [f for f in os.listdir(data_dir) 
                      if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
        num_images = len(image_files)
        
        print(f"üìä –ù–∞–π–¥–µ–Ω–æ {num_images} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ")
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –º–∞–ª–µ–Ω—å–∫–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
        if num_images < 50:
            print("‚ö†Ô∏è  –û–±–Ω–∞—Ä—É–∂–µ–Ω –º–∞–ª–µ–Ω—å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç. –ü—Ä–∏–º–µ–Ω—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏:")
            
            # –£–º–µ–Ω—å—à–∞–µ–º batch size
            if config['training_config']['batch_size'] > num_images // 2:
                config['training_config']['batch_size'] = max(1, num_images // 3)
                print(f"  üîß Batch size: {config['training_config']['batch_size']}")
            
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
            config['augmentation_config']['horizontal_flip'] = 0.8
            config['augmentation_config']['rotation'] = 15
            config['augmentation_config']['brightness'] = 0.3
            config['augmentation_config']['contrast'] = 0.3
            print("  üîß –£—Å–∏–ª–µ–Ω—ã –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö")
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è transfer learning
            config['model_config']['freeze_backbone'] = True
            config['model_config']['freeze_epochs'] = 20
            print("  üîß –ó–∞–º–æ—Ä–æ–∑–∫–∞ backbone –Ω–∞ 20 —ç–ø–æ—Ö")
            
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
            config['training_config']['epochs'] = 200
            print("  üîß –£–≤–µ–ª–∏—á–µ–Ω–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö: 200")
            
            # –£—Å–∏–ª–∏–≤–∞–µ–º regularization
            config['training_config']['weight_decay'] = 0.001
            config['training_config']['dropout'] = 0.3
            print("  üîß –£—Å–∏–ª–µ–Ω–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è")
    
    return config

def create_minimal_dataset_structure(project_dir: str):
    """–°–æ–∑–¥–∞–Ω–∏–µ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–ª—è –º–∞–ª–µ–Ω—å–∫–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    
    data_dir = os.path.join(project_dir, "data", "curated")
    
    # –°–æ–∑–¥–∞—ë–º train –∏ val –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è YOLO
    train_dir = os.path.join(data_dir, "train")
    val_dir = os.path.join(data_dir, "val")
    
    os.makedirs(train_dir, exist_ok=True)
    os.makedirs(val_dir, exist_ok=True)
    
    # –ö–æ–ø–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    image_files = [f for f in os.listdir(data_dir) 
                  if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
    
    # 70% –≤ train, 30% –≤ val
    n_train = max(1, int(len(image_files) * 0.7))
    train_files = image_files[:n_train]
    val_files = image_files[n_train:]
    
    print(f"üì¶ –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã: {len(train_files)} train, {len(val_files)} val")
    
    # –ö–æ–ø–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã
    import shutil
    for f in train_files:
        src = os.path.join(data_dir, f)
        dst = os.path.join(train_dir, f)
        if os.path.exists(src) and not os.path.exists(dst):
            shutil.copy2(src, dst)
    
    for f in val_files:
        src = os.path.join(data_dir, f)
        dst = os.path.join(val_dir, f)
        if os.path.exists(src) and not os.path.exists(dst):
            shutil.copy2(src, dst)
    
    return len(train_files), len(val_files)

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è"""
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    logger = setup_logger("training", level=logging.INFO)
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –¥–µ—Ç–µ–∫—Ü–∏–∏ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π AIinDrive")
    
    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        config_path = "configs/training_config.yaml"
        if not os.path.exists(config_path):
            logger.error(f"‚ùå –§–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {config_path}")
            logger.info("üí° –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–Ω–∞—á–∞–ª–∞: python scripts/prepare_training.py")
            return
        
        config = load_config(config_path)
        logger.info(f"‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {config_path}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏ –∞–¥–∞–ø—Ç–∞—Ü–∏—è –¥–ª—è –º–∞–ª–µ–Ω—å–∫–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
        config = check_small_dataset_setup(config)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞—Ç–∞—Å–µ—Ç–∞
        project_dir = str(Path(__file__).parent.parent)
        n_train, n_val = create_minimal_dataset_structure(project_dir)
        
        if n_train == 0:
            logger.error("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è!")
            return
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ CUDA
        if torch.cuda.is_available():
            logger.info(f"üî• CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: {torch.cuda.get_device_name()}")
            device = torch.device("cuda")
        else:
            logger.info("üíª –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")
            device = torch.device("cpu")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç—Ä–µ–Ω–µ—Ä–∞
        logger.info("üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç—Ä–µ–Ω–µ—Ä–∞...")
        trainer = DamageTrainer(config, device=device)
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–µ
        logger.info("üìã –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è:")
        logger.info(f"  –ú–æ–¥–µ–ª—å: {config['model_config']['architecture']}")
        logger.info(f"  –ö–ª–∞—Å—Å–æ–≤: {config['model_config']['num_classes']}")
        logger.info(f"  Batch size: {config['training_config']['batch_size']}")
        logger.info(f"  –≠–ø–æ—Ö: {config['training_config']['epochs']}")
        logger.info(f"  Learning rate: {config['training_config']['learning_rate']}")
        logger.info(f"  Train images: {n_train}")
        logger.info(f"  Val images: {n_val}")
        
        # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
        logger.info("üéØ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...")
        best_model_path = trainer.train()
        
        logger.info("üéâ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        logger.info(f"üèÜ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model_path}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–µ
        training_info = {
            'timestamp': datetime.now().isoformat(),
            'config': config,
            'dataset_size': {'train': n_train, 'val': n_val},
            'best_model_path': best_model_path,
            'device': str(device)
        }
        
        info_path = os.path.join(project_dir, 'results', 'last_training_info.json')
        os.makedirs(os.path.dirname(info_path), exist_ok=True)
        with open(info_path, 'w', encoding='utf-8') as f:
            json.dump(training_info, f, indent=2, ensure_ascii=False)
        
        logger.info(f"üìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {info_path}")
        
    except Exception as e:
        logger.error(f"üí• –û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        raise

if __name__ == "__main__":
    main()
