#!/usr/bin/env python3
"""–û–±—É—á–µ–Ω–∏–µ –Ω–∞ –±–æ–ª—å—à–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π"""

import os
import sys
import yaml
import torch
import json
from pathlib import Path
from datetime import datetime
from ultralytics import YOLO

def train_on_large_dataset(project_dir: str, dataset_yaml_path: str, config_path: str):
    """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –±–æ–ª—å—à–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ"""
    
    print("üöÄ –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –±–æ–ª—å—à–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π")
    print("=" * 70)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    with open(config_path, 'r') as f:
        config = yaml.load(f, Loader=yaml.FullLoader)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∞—Ç–∞—Å–µ—Ç
    if not os.path.exists(dataset_yaml_path):
        print(f"‚ùå –î–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {dataset_yaml_path}")
        return None
    
    print(f"üìä –î–∞—Ç–∞—Å–µ—Ç: {dataset_yaml_path}")
    
    # –°—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    with open(dataset_yaml_path, 'r') as f:
        dataset_config = yaml.load(f, Loader=yaml.FullLoader)
    
    dataset_path = dataset_config['path']
    train_images_path = os.path.join(dataset_path, 'images')
    
    if os.path.exists(train_images_path):
        image_files = [f for f in os.listdir(train_images_path) 
                      if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
        num_images = len(image_files)
        print(f"üìà –ù–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {num_images}")
    else:
        num_images = 0
        print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥—Å—á–∏—Ç–∞—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    
    if num_images < 100:
        print("‚ö†Ô∏è –ú–∞–ª–µ–Ω—å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏")
        # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –º–∞–ª–µ–Ω—å–∫–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
        config['training_config']['batch_size'] = min(8, max(1, num_images // 10))
        config['training_config']['epochs'] = 150
    
    # –í—ã–±–∏—Ä–∞–µ–º –º–æ–¥–µ–ª—å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
    if num_images < 500:
        model_name = 'yolov8n.pt'  # Nano –¥–ª—è –º–∞–ª–µ–Ω—å–∫–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
        print("üèóÔ∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º YOLOv8n (nano) –¥–ª—è –º–∞–ª–µ–Ω—å–∫–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞")
    elif num_images < 2000:
        model_name = 'yolov8s.pt'  # Small –¥–ª—è —Å—Ä–µ–¥–Ω–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
        print("üèóÔ∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º YOLOv8s (small) –¥–ª—è —Å—Ä–µ–¥–Ω–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞")
    else:
        model_name = 'yolov8m.pt'  # Medium –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
        print("üèóÔ∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º YOLOv8m (medium) –¥–ª—è –±–æ–ª—å—à–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
    model = YOLO(model_name)
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"üî• –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    
    if device == 'cuda':
        print(f"   GPU: {torch.cuda.get_device_name()}")
        print(f"   –ü–∞–º—è—Ç–∏: {torch.cuda.get_device_properties(0).total_memory // 1024**3} GB")
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
    training_args = {
        'data': dataset_yaml_path,
        'epochs': config['training_config']['epochs'],
        'imgsz': config['model_config']['input_size'],
        'batch': config['training_config']['batch_size'],
        'patience': config['training_config']['patience'],
        'save': True,
        'cache': False,
        'device': device,
        'workers': config['data_config']['num_workers'],
        'project': os.path.join(project_dir, 'results', 'large_dataset_training'),
        'name': f'car_damage_v2_{num_images}imgs',
        'exist_ok': True,
        'verbose': True,
        
        # –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
        'flipud': config['augmentation_config'].get('vertical_flip', 0.1),
        'fliplr': config['augmentation_config'].get('horizontal_flip', 0.5),
        'degrees': config['augmentation_config'].get('rotation', 10),
        'translate': config['augmentation_config'].get('translate', 0.1),
        'scale': config['augmentation_config'].get('scale', 0.5),
        'shear': config['augmentation_config'].get('shear', 2.0),
        'perspective': config['augmentation_config'].get('perspective', 0.0),
        'hsv_h': config['augmentation_config'].get('hue', 0.015),
        'hsv_s': config['augmentation_config'].get('saturation', 0.7),
        'hsv_v': config['augmentation_config'].get('brightness', 0.4),
        'mixup': config['augmentation_config'].get('mixup', 0.15),
        'mosaic': config['augmentation_config'].get('mosaic', 1.0),
        'copy_paste': config['augmentation_config'].get('copy_paste', 0.3),
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
        'lr0': config['training_config']['learning_rate'],
        'lrf': 0.01,
        'momentum': 0.937,
        'weight_decay': config['training_config']['weight_decay'],
        'warmup_epochs': config['training_config'].get('warmup_epochs', 3),
        'warmup_momentum': 0.8,
        'warmup_bias_lr': 0.1,
        'cos_lr': config['training_config'].get('cos_lr', False),
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        'val': True,
        'conf': config['validation_config']['conf_threshold'],
        'iou': config['validation_config']['iou_threshold'],
        'max_det': config['validation_config'].get('max_det', 300),
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        'plots': True,
        'save_period': config['logging_config'].get('save_period', -1)
    }
    
    print("\nüìã –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è:")
    print(f"  –ú–æ–¥–µ–ª—å: {model_name}")
    print(f"  –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ: {num_images}")
    print(f"  –≠–ø–æ—Ö: {training_args['epochs']}")
    print(f"  Batch size: {training_args['batch']}")
    print(f"  Learning rate: {training_args['lr0']}")
    print(f"  –†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {training_args['imgsz']}")
    print(f"  Workers: {training_args['workers']}")
    print(f"  Patience: {training_args['patience']}")
    
    # –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
    try:
        print(f"\nüéØ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...")
        print(f"‚è∞ –í—Ä–µ–º—è –Ω–∞—á–∞–ª–∞: {datetime.now().strftime('%H:%M:%S')}")
        
        results = model.train(**training_args)
        
        print(f"\nüéâ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        print(f"‚è∞ –í—Ä–µ–º—è –æ–∫–æ–Ω—á–∞–Ω–∏—è: {datetime.now().strftime('%H:%M:%S')}")
        
        # –ü—É—Ç—å –∫ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
        best_model_path = os.path.join(
            project_dir, 'results', 'large_dataset_training',
            f'car_damage_v2_{num_images}imgs', 'weights', 'best.pt'
        )
        
        print(f"üèÜ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model_path}")
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
        if os.path.exists(best_model_path):
            print("\nüîç –ó–∞–ø—É—Å–∫ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏...")
            best_model = YOLO(best_model_path)
            val_results = best_model.val(data=dataset_yaml_path)
            
            print("‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
            print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {val_results}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–µ
        training_info = {
            'timestamp': datetime.now().isoformat(),
            'dataset_path': dataset_yaml_path,
            'dataset_size': num_images,
            'model_name': model_name,
            'config_used': config,
            'training_args': training_args,
            'best_model_path': best_model_path,
            'device': device,
            'status': 'completed'
        }
        
        info_path = os.path.join(
            project_dir, 'results', 'large_dataset_training',
            f'training_info_{num_images}imgs.json'
        )
        
        os.makedirs(os.path.dirname(info_path), exist_ok=True)
        with open(info_path, 'w', encoding='utf-8') as f:
            json.dump(training_info, f, indent=2, ensure_ascii=False)
        
        print(f"üìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–µ: {info_path}")
        
        return best_model_path
        
    except Exception as e:
        print(f"\nüí• –û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {str(e)}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –æ—à–∏–±–∫–µ
        error_info = {
            'timestamp': datetime.now().isoformat(),
            'dataset_path': dataset_yaml_path,
            'dataset_size': num_images,
            'model_name': model_name,
            'config_used': config,
            'error': str(e),
            'status': 'failed'
        }
        
        error_path = os.path.join(
            project_dir, 'results', 'large_dataset_training',
            f'training_error_{num_images}imgs.json'
        )
        
        os.makedirs(os.path.dirname(error_path), exist_ok=True)
        with open(error_path, 'w', encoding='utf-8') as f:
            json.dump(error_info, f, indent=2, ensure_ascii=False)
        
        print(f"‚ùå –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –æ—à–∏–±–∫–µ: {error_path}")
        raise

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    
    project_dir = "/Users/bekzat/projects/AIinDrive"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
    synthetic_dataset_yaml = os.path.join(
        project_dir, "data", "synthetic_car_damage", "dataset.yaml"
    )
    
    large_config_path = os.path.join(
        project_dir, "configs", "large_dataset_training_config.yaml"
    )
    
    if not os.path.exists(synthetic_dataset_yaml):
        print("‚ùå –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        print("üí° –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–Ω–∞—á–∞–ª–∞: python scripts/setup_large_datasets.py")
        return
    
    if not os.path.exists(large_config_path):
        print("‚ùå –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
        print("üí° –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–Ω–∞—á–∞–ª–∞: python scripts/setup_large_datasets.py")
        return
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
    best_model_path = train_on_large_dataset(
        project_dir, 
        synthetic_dataset_yaml, 
        large_config_path
    )
    
    if best_model_path:
        print(f"\nüèÜ –û–±—É—á–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        print(f"üìÅ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model_path}")
        print(f"\nüí° –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:")
        print(f"1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ results/large_dataset_training/")
        print(f"2. –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π—Ç–µ –º–æ–¥–µ–ª—å: python scripts/test_large_model.py")
        print(f"3. –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–π—Ç–µ –≤ CV –∞–Ω–∞–ª–∏–∑ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏")

if __name__ == "__main__":
    main()
